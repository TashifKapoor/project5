# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XUaelAWxRiw3Qmn24yBT_evzTgzEAKmk
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras

df = pd.read_csv("/content/train_tweet.csv")

df.head()

df.tail()

df.info()

df.describe(include = 'all')

df.isnull().sum()

df.columns

df.shape

df.size

df.nunique()

df = df.drop('id',axis=1)

df.head()

labels = pd.get_dummies(df.label)
labels.columns = ["negative", "positive"]
labels.head(5)

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
tokenizer = Tokenizer(num_words = 8150, lower = True, split = " ", oov_token = "~")
tokenizer.fit_on_texts(df["tweet"])

word_index = tokenizer.word_index
len(word_index)

print(list(word_index.keys())[:100])

df["tweet"] = tokenizer.texts_to_sequences(df["tweet"])

df.head(3)

len(df.tweet[0]), len(df.tweet[1]), len(df.tweet[2])

tweets = pad_sequences(df["tweet"])

tweets[0].shape, tweets[1].shape, tweets[2].shape

tweets.shape

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(tweets, labels, test_size = 0.2)

X_train.shape, y_train.shape, X_test.shape, y_test.shape

X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.2)

print(f"""
Training set: tweets = {X_train.shape}, labels = {y_train.shape},
Validation set: tweets = {X_valid.shape}, labels = {y_valid.shape},
Test set: tweets = {X_test.shape}, labels = {y_test.shape}
""")

from tensorflow import keras
from keras.layers import Embedding, GRU, Dense, Dropout
from keras.activations import relu
from keras.initializers import he_normal, glorot_normal
from keras.regularizers import l1
from keras.optimizers import Adam
from keras.utils import plot_model

# Assuming you have three classes (negative, neutral, positive)
num_classes = 3

model = keras.models.Sequential([
    Embedding(input_dim=8150, output_dim=32),
    GRU(128),
    Dense(128, activation=relu, kernel_initializer=he_normal(), kernel_regularizer=l1(0.01)),
    Dropout(0.5),
    Dense(num_classes, activation='softmax', kernel_initializer=glorot_normal())
])

model.summary()

model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

# Optionally, you can plot the model architecture
plot_model(model, to_file='sentiment_analysis_model.png', show_shapes=True, show_layer_names=True)

# One-hot encode your labels
from keras.utils import to_categorical
y_train_encoded = to_categorical(y_train, num_classes=num_classes)
y_valid_encoded = to_categorical(y_valid, num_classes=num_classes)

# Train the model
history = model.fit(
    y_train_encoded, y_valid_encoded,
    epochs=500, validation_data=(X_valid, y_valid_encoded),
    callbacks=[keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)]
)

# Evaluate the model on the validation set
loss, accuracy = model.evaluate(X_valid, y_valid_encoded)
print(f'Validation Loss: {loss:.4f}')
print(f'Validation Accuracy: {accuracy*100:.2f}%')







